{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from collections.abc import MutableMapping\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data\n",
    "\n",
    "The following code block calls the LAWIS API and saves all incidents in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the initial parameters for the API request\n",
    "url = \"https://lawis.at/lawis_api/public/incident\"\n",
    "params = {\n",
    "    \"startDate\": \"1900-01-01\",\n",
    "    \"endDate\": \"2023-03-07\",\n",
    "}\n",
    "headers = {\"Accept\": \"application/json\"}\n",
    "\n",
    "# make the initial request and get the incident IDs\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "incident_ids = [incident[\"id\"] for incident in response.json()]\n",
    "\n",
    "# initialize an empty list to store the incident data\n",
    "incident_data = []\n",
    "\n",
    "# define a recursive function to flatten the incident data\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    \"\"\"\n",
    "    Recursively flattens a nested dictionary into a flat dictionary by combining nested keys\n",
    "    with a separator to create unique keys.\n",
    "\n",
    "    Parameters:\n",
    "        d (dict): The dictionary to flatten.\n",
    "        parent_key (str): The prefix to add to flattened keys.\n",
    "        sep (str): The separator to use between keys in the flattened dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: The flattened dictionary.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        # combine the current key with the parent key using the separator\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "\n",
    "        # if the value is a dictionary, recursively call the function to flatten it\n",
    "        if isinstance(v, MutableMapping):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            # if the value is not a dictionary, add it to the list of items as a tuple\n",
    "            items.append((new_key, v))\n",
    "    # convert the list of items back to a dictionary and return it\n",
    "    return dict(items)\n",
    "\n",
    "for incident_id in incident_ids:\n",
    "    url = f\"https://lawis.at/lawis_api/public/incident/{incident_id}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    incident = response.json()\n",
    "    \n",
    "    # flatten all dictionaries into separate columns\n",
    "    flat_incident = flatten_dict(incident)\n",
    "    \n",
    "    # add the flattened incident to the list\n",
    "    incident_data.append(flat_incident)\n",
    "\n",
    "# convert the incident data to a pandas DataFrame\n",
    "df = pd.DataFrame(incident_data)\n",
    "\n",
    "df.to_csv('data/avalanche_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "complete_df = pd.read_csv('data/avalanche_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep relevant columns\n",
    "df = complete_df[['id', 'date', 'involved_dead', 'involved_injured',\n",
    "       'involved_uninjured', 'involved_sweeped', 'involved_buried_partial',\n",
    "       'involved_buried_total', 'danger_rating_level', 'danger_rating_text', 'avalanche_extent_length',\n",
    "       'avalanche_extent_width', 'avalanche_breakheight', 'avalanche_type_id',\n",
    "       'avalanche_type_text', 'avalanche_size_id', 'avalanche_size_text',\n",
    "       'location_name', 'location_longitude', 'location_latitude',\n",
    "       'location_aspect_id', 'location_aspect_text', \n",
    "       'location_country_code', 'location_elevation',\n",
    "       'location_slope_angle']].copy()\n",
    "\n",
    "total_incidents = len(df)\n",
    "involved_incidents = len(df[(df['involved_dead'] > 0) | (df['involved_injured'] > 0) | (df['involved_uninjured'] > 0) | (df['involved_sweeped'] > 0)])\n",
    "\n",
    "total_deaths = df['involved_dead'].sum()\n",
    "total_injured = df['involved_injured'].sum()\n",
    "\n",
    "\n",
    "# print basic stats to get an idea about the dataset\n",
    "print(f'Incidents in Dataset: {total_incidents} \\nIncidents involving people: {involved_incidents} \\nDeaths: {total_deaths}\\nInjured {total_injured}')\n",
    "\n",
    "# show the ten deadliest avalanches\n",
    "display(df.sort_values('involved_dead', ascending=False).head(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn that the dataset contains 3976 incidents of which 2342 involved people.\n",
    "896 people were injured by the avalanches in the dataset and 586 people lost their lives.\n",
    "\n",
    "The most devastating avalanche in the dataset was the 1999 GaltÃ¼r avalanche which killed 31 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to a datetime object\n",
    "df['date'] = pd.to_datetime(df['date'], utc=True)\n",
    "\n",
    "\n",
    "\n",
    "# Get the number of incidents by year\n",
    "incidents_by_year = df.groupby(df['date'].dt.year).size()\n",
    "\n",
    "# Get the number of fatal incidents by year\n",
    "fatal_incidents_by_year = df[df['involved_dead'] > 0].groupby(df['date'].dt.year).size()\n",
    "\n",
    "# Group the data by year and sum the 'involved_dead' column for each group\n",
    "fatalities_by_year = df.groupby(df['date'].dt.year)['involved_dead'].sum()\n",
    "\n",
    "\n",
    "# Create a barplot of the incidents by year\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows = 3, figsize=(10, 10))\n",
    "\n",
    "\n",
    "ax1.bar(incidents_by_year.index, incidents_by_year.values, color='steelblue')\n",
    "ax1.set_ylabel('Avalanche Incidents')\n",
    "ax1.set_title('Avalanche Incidents by Year')\n",
    "\n",
    "ax2.bar(fatal_incidents_by_year.index, fatal_incidents_by_year.values, color='steelblue')\n",
    "ax2.set_ylabel('Fatal Avalanche Incidents')\n",
    "ax2.set_title('Fatal Avalanche Incidents by Year')\n",
    "\n",
    "ax3.bar(fatalities_by_year.index, fatalities_by_year.values, color='steelblue')\n",
    "ax3.set_ylabel('Number of Avalanche Fatalities')\n",
    "ax3.set_title('Avalanche Fatalities by Year')\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LAWIS Database went online in 2015, so we can assume that the reporting became more complete since then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most number of dead and injured in danger level 3\n",
    "print(df.groupby('danger_rating_level')['involved_dead','involved_injured','involved_buried_partial','involved_buried_total'].sum())\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby('danger_rating_level').sum()[['involved_dead', 'involved_injured', 'involved_uninjured']].dropna()\n",
    "\n",
    "grouped.plot(kind='bar', stacked=True)\n",
    "\n",
    "grouped_relative = grouped.div(grouped.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "grouped_relative.plot(kind='bar', stacked=True)\n",
    "\n",
    "# Add chart title and axis labels\n",
    "plt.title('Stacked Bar Chart by Danger Rating (Relative Distribution)')\n",
    "plt.xlabel('Danger Rating Level')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Map\n",
    "\n",
    "I got interested in creating my own (animated) map of the incidents in the dataset. So I try to learn D3.js and leaflet.js basics. As a data source I need to save a csv with the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe we can visualize the data in a circular histogram to show when most of the avalanches occur.\n",
    "# Exploratory Python Plot\n",
    "import numpy as np\n",
    "\n",
    "# Create a new column that extracts just the date (ignoring the time)\n",
    "df['date_only'] = df['date'].dt.dayofyear\n",
    "\n",
    "# Group the data by date and count the number of avalanches\n",
    "avalanche_count = df.groupby('date_only')['id'].count()\n",
    "\n",
    "# Group the data by date and sum the number of involved_dead\n",
    "involved_dead_count = df.groupby('date_only')['involved_dead'].sum()\n",
    "\n",
    "# Plot the two histograms on the same axis\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(avalanche_count.index, avalanche_count.values, label='Avalanches')\n",
    "ax.bar(involved_dead_count.index, involved_dead_count.values, label='Involved Dead')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fill NaN values in 'involved_dead' column with 0\n",
    "df['involved_dead'] = df['involved_dead'].fillna(0)\n",
    "\n",
    "# fill NaN values in 'involved_injured' column with 0\n",
    "df['involved_injured'] = df['involved_injured'].fillna(0)\n",
    "\n",
    "# add a new column for day of year\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "# adjust day_of_year for leap years\n",
    "df['day_of_year'] -= (df['day_of_year'] >= 60) & df['date'].dt.is_leap_year\n",
    "df['day_of_year'] = (df['day_of_year'] + 184) % 365 \n",
    "\n",
    "# add a new column for day count\n",
    "df['daycount'] = df.groupby('day_of_year').cumcount() + 1\n",
    "\n",
    "# create the scatter plot\n",
    "plt.scatter(df['day_of_year'], df['daycount'], s=df['involved_dead']+0.1*5)\n",
    "\n",
    "# add axis labels and title\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Day Count')\n",
    "plt.title('Scatter Plot of Involved Dead')\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdata = df[['id', 'date', 'involved_dead', 'involved_injured', 'danger_rating_level', 'danger_rating_text', 'location_name', 'location_longitude', 'location_latitude', 'day_of_year', 'daycount']].copy()\n",
    "#mapdata = mapdata.dropna()\n",
    "#mapdata = mapdata[mapdata['date'] > '2015-01-01']\n",
    "mapdata['danger_rating_level'] = mapdata['danger_rating_level'].fillna(0).astype(int)\n",
    "mapdata['danger_rating_text'] = mapdata['danger_rating_text'].fillna('not assigned')\n",
    "mapdata.sort_values(by='date', inplace=True)\n",
    "mapdata = mapdata.reset_index(drop=True)\n",
    "mapdata.index.name = 'index'\n",
    "mapdata.to_csv('D3/avalanche_data.csv', index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runningSum = df.groupby('date').sum()[['involved_dead', 'involved_injured', 'involved_uninjured']].dropna()\n",
    "runningSum = runningSum.cumsum()\n",
    "runningSum.plot.area(stacked=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
